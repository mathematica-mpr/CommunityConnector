---
output:
  html_document: default
  pdf_document: default
---
```{r}
rm(list = ls())
```

```{r}
source('utilities.R')

# TODO: also parse down the demographic variables?
data_dictionary <- read.csv('../../data/final_data_dictionary.csv')
data <- read.csv('../../data/final_data.csv')
data_dictionary %>% 
  filter(modifiable == 1) %>% 
  dplyr::select(column_name, higher_better)
```

```{r}
# select outcome
outcomes <- data_dictionary %>% 
  filter(outcome == 1) %>% 
  dplyr::select(column_name) %>% 
  mutate(column_name = as.character(column_name)) %>% 
  pull()
```

```{r Methodology options}
opts <- data.frame("use_sdoh_scores" = c(1, 0, 0),
                      "use_sdoh_raw" = c(0, 1, 0),
                      "use_dems" = c(1, 1, 1))
# opts <- data.frame("use_sdoh_scores" = c(0),
#                       "use_sdoh_raw" = c(1),
#                       "use_dems" = c(1))

# TODO: not sure if this cross join is going to work out with the functions later because we want to run the RF model only once for each row and get the prediction and proximity out of it
# methods = c("rf proximity","euclidean","rf prediction","lasso","gbm prediction")
# methods = c("rf proximity","euclidean","rf prediction","lasso")
methods = c("lasso")
opts <- merge(opts, data.frame("methodology" = methods), all = TRUE)

opts$meth_num <- row_number(opts$use_dems)
# TODO: eventually will add a remove modifiable, relevant option
# TODO: other prediction algorithms: rf prediction, gbm prediction
# TODO: what other metrics to use to evaluate?

# both the aggregate score and the inputs should not be used at the same time
print(opts[opts$use_sdoh_scores & opts$use_sdoh_raw,])
print(opts)
```

```{r}
# full_results <- apply(opts, 1, implement_methodology, outcomes, data, data_dictionary)
full_results <- apply(opts, 1, implement_methodology, outcomes, data, data_dictionary, num_counties = 1)
```

```{r}
results <- bind_rows(full_results, .id = "column_label")

# average metrics across county by outcome to evaluate results of different methodologies
summ_results <- results %>% 
  group_by(use_outcome, methodology, meth_num, column_label) %>% 
  dplyr::summarise_all(funs(median)) %>% 
  arrange(use_outcome) %>% 
  dplyr::select(-column_label, -county_num)
summ_results
```

```{r}
# find best methodology across a number of metrics and outcomes 
for(col in names(summ_results)){
  if(!col %in% c("methodology","meth_num","use_outcome")){
    if(col != "pct_reduced_sd"){
      best <- summ_results %>% 
        group_by(use_outcome) %>% 
        slice(which.min(!!rlang::sym(col))) %>% 
        dplyr::select(meth_num)  
    } else {
      best <- summ_results %>% 
        group_by(use_outcome) %>% 
        slice(which.max(!!rlang::sym(col))) %>% 
        dplyr::select(meth_num)
    }
    
    print(best)
    
    # which one is the most common?
    best_num <- best %>% 
      group_by(meth_num) %>% 
      dplyr::summarise(n = n()) %>% 
      arrange(desc(n)) %>% 
      slice(1) %>% 
      dplyr::select(meth_num)
    print(paste0("Best methodology for ", col, ": ", best_num))
  }
}


# Euclidean distance is the best so far between that & rf proximity
# between euclidean, rf proximity, and rf prediction, best is 8 (RF pred) - raw data
# Euc vs. rf proximity vs. rf prediction vs. un-CV lasso (0.1): 8 (RF pred) or 11 (Lasso)
# Seems like using all of the raw data is always best
# Euc vs. rf proximity vs. rf prediction vs. un-CV lasso (0.1) vs. GBM: 8 (RF pred) or 11(Lasso)
# another time, definitely 11 Lasso
# using CV to predict results, 11 (Lasso) is the best by far - raw data. 2nd best is 10
# However, we don't necessarily want the BEST prediction model, but maybe we can try removing the modifiable variables and re-running?
opts

```