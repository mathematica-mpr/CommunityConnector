```{r Set up}
rm(list = ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
source('utilities.R')
```

```{r Read data}
data_dictionary <- read.csv('../../data/final_data_dictionary.csv')
data <- read.csv('../../data/final_data.csv')

outcomes <- data_dictionary %>% 
  filter(outcome == 1) %>% 
  dplyr::select(column_name) %>% 
  mutate(column_name = as.character(column_name)) %>% 
  pull()
```

```{r One-row data frame with all parameters}
method <- "lasso euclidean all"
opts <- data.frame("use_sdoh_scores" = 0,
                   "use_sdoh_raw" = 1,
                   "use_dems" =  1,
                   "remove_modifiable" = 0,
                   "methodology" = method,
                   "meth_num" = 1)
if(method %in% c("lasso euclidean all","lasso euclidean dem")){
  num_cols <- unlist(lapply(data, is.numeric))
  data[,num_cols] <- scale(data[,num_cols])
}
```

```{r Implement methodology for final choice}
upd_implement_methodology <- function(row, outcomes, data, data_dictionary, all_outcome_params = NA, num_counties = NA){
  
  # Define variables from opts dataframe
  use_sdoh_scores <- as.numeric(row$use_sdoh_scores)
  use_sdoh_raw <- as.numeric(row$use_sdoh_raw)
  use_dems <- as.numeric(row$use_dems)
  remove_modifiable <- as.numeric(row$remove_modifiable)
  methodology <- as.character(row$methodology)
  meth_num <- as.numeric(row$meth_num)
  
  # empty list that will be filled in with distance matrices
  distance_list <- list()
  
  start_time <- Sys.time()
  print(methodology)
  print(start_time)
  
  # Loop through all outcomes
  n <- 1
  for(use_outcome in outcomes){
    print(paste("Outcome:", use_outcome))
    
    orig_data <- data %>% 
      filter(!is.na(!!rlang::sym(use_outcome)))
    
    # Select variables to match on, limit data to these variables, and replace NAs
    use_data <- select_distance_columns(data = orig_data, data_dictionary = data_dictionary,
                                        sdoh_scores = use_sdoh_scores, sdoh_raw = use_sdoh_raw,
                                        outcome = use_outcome, dem = use_dems)
    
    # this chunk makes the code more efficient so we don't have to tune the same model multiple times for the same outcome/variables
    # instead, we can input tuning parameters from a past best model
    n_rows <- nrow(all_outcome_params)
    model_params <- NA
    if(length(n_rows) != 0){
      out <- use_outcome
      model_params <- all_outcome_params %>% 
        filter(use_outcome == out)
    }
    
    # Get distance matrix using methodology specified
    dist_results <- county_distance(use_data, orig_data$fips, data_dictionary, methodology, use_outcome, remove_modifiable, model_params)
    # all outputs from distance methodology
    distancem <- dist_results[1][[1]]
    mse <- dist_results[2][[1]]
    # best tuning parameters to save in case we need to re-run
    mtry <- dist_results[3][[1]]
    alpha <- dist_results[4][[1]]
    min_lambda <- dist_results[5][[1]]
    coefs_df <- dist_results[6][[1]]
    
    # use total number of counties if we didn't specify how many to use
    if(is.na(num_counties)){
      n_counties <- dim(distancem)[1]
    } else {
      n_counties <- num_counties
    }
    
    # Loop through counties to get spread metrics for each county's top 5 most similar
    for(county_num in c(1:n_counties)){
      data <- select_county(orig_data, distancem, county_num)
      
      ## Evaluate the methodology:
      # Look at the radar charts in order of county similarity
      # How similar are health outcomes of the top 5 most similar or similar within a certain distance?
      # They should have a median close to the county in question
      results <- evaluate_methodology(data, use_outcome)
      results$metric <- rownames(results)
      results_df <- results %>% 
        spread(metric, sds) %>% 
        mutate("mse" = mse,
               "county_num" = county_num,
               "use_outcome" = use_outcome,
               "methodology" = methodology,
               "meth_num" = meth_num,
               "mtry" = mtry,
               "alpha" = alpha,
               "min_lambda" = min_lambda)
      
      # append results of all counties
      if(use_outcome == outcomes[1]){
        full_results <- results_df
      } else {
        full_results <- full_results %>% 
          rbind(results_df)
      }
      
    }
    
    # append coefficients of all models
    if(n == 1){
      coefs_all <- coefs_df
    } else {
      coefs_all <- coefs_all %>% 
        merge(coefs_df, by = "name", all.x = TRUE, all.y = TRUE)
    }
    names(coefs_all)[n+1] <- use_outcome
    distance_list[[n]] <- distancem
    n <- n+1
    
  }
  
  end_time <- Sys.time()
  print(paste0("Time elapsed: ", end_time - start_time))
  start_time <- end_time
  
  return(list(full_results,distance_list, coefs_all))
}
```

Do we want to remove variables manually?
# The justification behind this:
# For example, elevation shows up in some of the models
# Elevation is not causal, but there must be a correlation
# I worry that elevation is correlated to other variables, perhaps a modifiable (walkability) and another nonmodifiable, or something
# Elevation is sucking up the impact of these variables, which is fine for prediction
# But we want to use the coefficients of the modifiable and nonmodifiable variables separately
# If this is the case, keeping elevation would remove our ability to parse out these two impacts

# Decided not to remove variables manually based on previous analysis - not impactful at least for overobese_pct outcome
# If we decide to use another outcome, may want to test again

Do we want to normalize variables prior to the model? Yes - the changes seemed to make sense

```{r}
results <- upd_implement_methodology(opts, outcomes, data, data_dictionary)

# look at alpha and lambdas selected
# lambda = 0 = OLS
# with increase in lambda, bias increases & variance decreases
# alpha = 0 = ridge, = 1 = LASSO
# ridge keeps all variables. could keep all for larger values of alpha too if they are all important
# lambda penalizes the sum of the coefficients

results[1][[1]] %>% 
  dplyr::select(use_outcome, methodology, alpha, min_lambda) %>% 
  unique()
```

```{r Look at coefficients from all models}
coefs_df <- results[3][[1]]

View(coefs_df)
# probably want to remove completely:
# although if we kept them in with more data, they would probably not be selected
# Elevation, budget_emergency, pct_vacant

colSums(!is.na(coefs_df))
write.csv(coefs_df, '../../data/lasso_coefficients.csv',row.names = FALSE)

# which matrices would be most general/incorporate the most data
# looking at the variables used, overobese_pct definitely uses the most
# followed by pct obese & the std spend vars, which we won't want to use
# let's just use the distance matrix for overobese_pct for now
# this also is a lower level outcome than the others (i.e. impacts them down the road, so makes sense to use)
```

```{r Output final distance matrix}
# use the 8th because this is the overobese_pct outcome model results
final_distance <- as.data.frame(as.matrix(results[2][[1]][[8]]))
final_distance$sim_county <- rownames(final_distance)
head(final_distance)

# find the minimum distance for each county that is not 0
min_not_0 <- function(x){
  x <- as.numeric(x)
  x <- x[x!=0]
  min(x)
}
apply(final_distance, 1, min_not_0)

# 8031 is denver - which is it most similar to?
final_distance %>% 
  mutate(distance = !!rlang::sym('8031')) %>% 
  dplyr::select(distance, sim_county) %>%
  filter(distance < 1 & distance != 0)
#8005 - part of denver county area - makes sense!

# the variables used in at least 4/8 models
common_coefs <- as.character(coefs_df[which(rowSums(!is.na(coefs_df))>=5),"name"])
View(coefs_df %>% 
  filter(name %in% common_coefs))

# output distance matrix with FIPS code as identifier
write.csv(final_distance, '../../data/final_distance.csv')
```